import requests
import json
import time
import os
import sys

project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

print("Project root:", project_root)
print("sys.path:", sys.path)


from src.config import paths

# --- [1] ì„¤ì • ---
SAVE_DIR = paths.NEWS_DATA_DIR

# --- [2] í•„í„°ë§ í•¨ìˆ˜ (Gatekeeper - ìˆ˜ì •ë¨) ---
def should_collect(news_item):
    """
    ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í• ì§€ ë§ì§€ ê²°ì •í•˜ëŠ” í•¨ìˆ˜.
    Trueë¥¼ ë°˜í™˜í•˜ë©´ ìˆ˜ì§‘, Falseë©´ ê±´ë„ˆëœ€.
    """
    title = news_item.get('title', '').lower() # ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ë¹„êµ
    
    # [ìˆ˜ì • 1] ADMIN ì²´í¬ ì‚­ì œ (ìœ ì €ê°€ ì˜¬ë¦° ì–‘ì§ˆì˜ ë‰´ìŠ¤ë„ ìˆ˜ì§‘í•˜ê¸° ìœ„í•´)
    # if news_item.get('author_role') != 'ADMIN': ... (ì‚­ì œë¨)

    # [ìˆ˜ì • 2] ë…¸ì´ì¦ˆ í‚¤ì›Œë“œ ê°•í™” (í•œêµ­ì–´ + ì˜ì–´)
    # ë¡œê·¸ì— ë³´ì˜€ë˜ '(ì¹´ë”ë¼)' ê°™ì€ ì°Œë¼ì‹œë‚˜ ê°€ì‹­ì„± í‚¤ì›Œë“œë¥¼ ì œì™¸í•©ë‹ˆë‹¤.
    noise_keywords = [
        # 1. ì‹ ë¢°ë„ ë‚®ì€ ì •ë³´
        "ì°Œë¼ì‹œ", "ë£¨ë¨¸", "rumor",
        
        # 2. ì—°ì˜ˆ/ê°€ì‹­ (íˆ¬ìì™€ ë¬´ê´€í•œ)
        "ì—´ì• ", "ê²°ë³„", "dating", "scandal", "í­ë¡œ", "ì¶©ê²©",
        
        # 3. ìŠ¤í¬ì¸  (ë‹¨, ë¹„ì¦ˆë‹ˆìŠ¤ ë‰´ìŠ¤ëŠ” ì œì™¸ë˜ë„ë¡ ì£¼ì˜)
        "ê²½ê¸° ê²°ê³¼", "match result", "highlight"
    ]
    
    # ì œëª©ì— ë…¸ì´ì¦ˆ í‚¤ì›Œë“œê°€ ìˆëŠ”ì§€ í™•ì¸
    for keyword in noise_keywords:
        if keyword in title:
            print(f"   ğŸ—‘ï¸ íŒ¨ìŠ¤ (ë…¸ì´ì¦ˆ/ê°€ì‹­): {news_item['title']}")
            return False
            
    # [ì˜µì…˜] ì œëª©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´(5ê¸€ì ë¯¸ë§Œ) íŒ¨ìŠ¤ (ex: "ëŒ€ë°•", "ì†ë³´")
    if len(news_item.get('title', '')) < 5:
        print(f"   ğŸ“ íŒ¨ìŠ¤ (ì œëª© ë„ˆë¬´ ì§§ìŒ): {news_item['title']}")
        return False

    return True

# --- [3] ë©”ì¸ ìˆ˜ì§‘ ë¡œì§ ---
def run_collector():
    # 1. ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ìš”ì²­ (ìµœì‹  20ê°œ)
    list_url = "https://api.saveticker.com/api/news/list?page=1&page_size=20&sort=created_at_desc"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    print("ğŸ“¡ ìµœì‹  ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ìŠ¤ìº” ì¤‘... (í•„í„° ì™„í™”ë¨)")
    try:
        response = requests.get(list_url, headers=headers, timeout=10)
    except Exception as e:
        print(f"âŒ ì—°ê²° ì‹¤íŒ¨: {e}")
        return
    
    if response.status_code != 200:
        print("âŒ ë¦¬ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨")
        return

    news_list = response.json().get('news_list', [])
    
    count = 0
    for news_item in news_list:
        news_id = news_item['id']
        title = news_item['title']
        
        # 2. í•„í„°ë§ (ìˆ˜ì§‘ ê°€ì¹˜ íŒë‹¨)
        if not should_collect(news_item):
            continue 

        # 3. ì¤‘ë³µ í™•ì¸
        filename = SAVE_DIR / f"{news_id}.json"
        if os.path.exists(filename):
            print(f"   â­ï¸ ì´ë¯¸ ìˆìŒ: {title}")
            continue

        # 4. ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘
        detail_url = f"https://api.saveticker.com/api/news/detail/{news_id}"
        try:
            detail_res = requests.get(detail_url, headers=headers, timeout=5)
            
            if detail_res.status_code == 200:
                full_data = detail_res.json().get('news', {})
                
                # ë³¸ë¬¸ ì¶”ì¶œ
                content_text = ""
                raw_content = full_data.get('content', '')
                
                if isinstance(raw_content, list):
                    content_text = "\n".join([c.get('content', '') for c in raw_content if c.get('type') == 'text'])
                else:
                    content_text = str(raw_content)

                # ë°ì´í„° êµ¬ì¡°í•‘
                save_data = {
                    "id": news_id,
                    "title": full_data.get('title', ''),
                    "created_at": full_data.get('created_at', ''),
                    "content": content_text,
                    "source": full_data.get('source', ''),
                    "tags": [t.get('name') for t in full_data.get('tags', [])],
                    "author": full_data.get('author_name', 'Unknown') # ì‘ì„±ì ì •ë³´ ì¶”ê°€
                }

                with open(filename, "w", encoding="utf-8") as f:
                    json.dump(save_data, f, ensure_ascii=False, indent=4)
                
                print(f"   âœ… ìˆ˜ì§‘ ì™„ë£Œ: {title}")
                count += 1
                time.sleep(0.3) 
        except Exception as e:
            print(f"   âš ï¸ ìƒì„¸ ìˆ˜ì§‘ ì¤‘ ì—ëŸ¬: {title} ({e})")
            continue
        
    print(f"\nğŸ‰ ì´ {count}ê°œì˜ ë‰´ìŠ¤ë¥¼ ìƒˆë¡œ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    run_collector()