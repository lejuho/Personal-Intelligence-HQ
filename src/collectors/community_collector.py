import requests
import json
import time
import os
import sys

current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(os.path.dirname(os.path.dirname(current_dir)))
sys.path.append(project_root)
# --- [ì„¤ì •] ---
from src.config import paths
SAVE_DIR = paths.COMMUNITY_DATA_DIR  # ë³„ë„ í´ë”ì— ì €ì¥
if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)

# --- [í•„í„°ë§] ---
def should_collect(post_item):
    """ì»¤ë®¤ë‹ˆí‹° ê¸€ í•„í„°ë§ (ë…¸ì´ì¦ˆ ì œê±°)"""
    title = post_item.get('title', '').lower()
    content = post_item.get('content', '').lower()
    
    # 1. ë„ˆë¬´ ì§§ì€ ê¸€(ì¡ë‹´) ì œì™¸
    if len(content) < 30: 
        print(f"   ğŸ—‘ï¸ íŒ¨ìŠ¤ (ë„ˆë¬´ ì§§ìŒ): {post_item['title']}")
        return False

    # 2. ë…¸ì´ì¦ˆ í‚¤ì›Œë“œ í•„í„°ë§ (ì •ì¹˜, ìŠ¤í¬ì¸  ë“±)
    noise_keywords = ["election", "vote", "soccer", "baseball"]
    check_text = title + " " + content
    for k in noise_keywords:
        if k in check_text:
            print(f"   ğŸ—‘ï¸ íŒ¨ìŠ¤ (ë…¸ì´ì¦ˆ): {post_item['title']}")
            return False
            
    return True

def run_community_collector():
    # ì»¤ë®¤ë‹ˆí‹° API (ì¹´í…Œê³ ë¦¬: user_news)
    list_url = "https://api.saveticker.com/api/community/list?page=1&page_size=20&category=user_news&sort=created_at_desc"
    
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    
    print("ğŸ—£ï¸ ì»¤ë®¤ë‹ˆí‹° ì—¬ë¡ (User News) ìŠ¤ìº” ì¤‘...")
    try:
        response = requests.get(list_url, headers=headers)
        if response.status_code != 200:
            print(f"âŒ ë¦¬ìŠ¤íŠ¸ ì‹¤íŒ¨ (Status: {response.status_code})")
            return

        # ì»¤ë®¤ë‹ˆí‹° APIì˜ í‚¤ëŠ” 'posts' ì…ë‹ˆë‹¤.
        posts = response.json().get('posts', [])
        
        count = 0
        for post in posts:
            post_id = post['id']
            title = post['title']
            
            # 1. í•„í„°ë§
            if not should_collect(post):
                continue

            # 2. ì¤‘ë³µ í™•ì¸
            filename = os.path.join(SAVE_DIR, f"{post_id}.json")
            if os.path.exists(filename):
                continue # ì¡°ìš©íˆ ë„˜ì–´ê°

            # 3. ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘ (ì„ íƒì‚¬í•­ì´ë‚˜, í™•ì‹¤í•œ ì €ì¥ì„ ìœ„í•´ í˜¸ì¶œ)
            detail_url = f"https://api.saveticker.com/api/community/detail/{post_id}"
            detail_res = requests.get(detail_url, headers=headers)
            
            if detail_res.status_code == 200:
                # ìƒì„¸ ë°ì´í„° êµ¬ì¡° í™•ì¸ í•„ìš” (ë³´í†µ 'post' í‚¤ ì•ˆì— ìˆìŒ)
                full_data = detail_res.json().get('post', post) # ì—†ìœ¼ë©´ ë¦¬ìŠ¤íŠ¸ ë°ì´í„° ì‚¬ìš©
                
                # ì €ì¥í•  ë°ì´í„° êµ¬ì¡°
                save_data = {
                    "id": post_id,
                    "title": full_data['title'],
                    "created_at": full_data['created_at'],
                    "content": full_data.get('content', ''),
                    "author": full_data.get('author_name', 'Unknown'),
                    "view_count": full_data.get('view_count', 0),
                    "likes": full_data.get('like_stats', {}).get('like_count', 0),
                    "source": "Saveticker Community"
                }

                with open(filename, "w", encoding="utf-8") as f:
                    json.dump(save_data, f, ensure_ascii=False, indent=4)
                
                print(f"   âœ… ìˆ˜ì§‘: {title}")
                count += 1
                time.sleep(0.5)

        if count > 0:
            print(f"ğŸ‰ ì´ {count}ê°œì˜ ì»¤ë®¤ë‹ˆí‹° ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")
        else:
            print("ğŸ’¤ ìƒˆë¡œìš´ ì»¤ë®¤ë‹ˆí‹° ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        print(f"âš ï¸ ì—ëŸ¬ ë°œìƒ: {e}")

if __name__ == "__main__":
    run_community_collector()