import requests
from bs4 import BeautifulSoup
import os
from datetime import datetime

# [ì„¤ì •]
SAVE_DIR = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "data", "ipo_data")
if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
}

def collect_korea_ipo():
    """38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì—ì„œ í•œêµ­ ê³µëª¨ì£¼ ì²­ì•½ ì¼ì • ìˆ˜ì§‘"""
    print("ğŸ‡°ğŸ‡· í•œêµ­ IPO ì¼ì •(38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜) ìˆ˜ì§‘ ì¤‘...")
    url = "http://www.38.co.kr/html/fund/index.htm?o=k"
    
    try:
        res = requests.get(url, headers=HEADERS, timeout=10)
        res.encoding = 'EUC-KR' # 38ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì€ ì¸ì½”ë”© ì£¼ì˜
        soup = BeautifulSoup(res.text, "html.parser")
        
        table = soup.find("table", summary="ê³µëª¨ì£¼ ì²­ì•½ì¼ì •")
        rows = table.find_all("tr")
        
        ipo_list = []
        for row in rows:
            cols = row.find_all("td")
            if len(cols) < 5: continue
            
            try:
                name = cols[0].text.strip()
                schedule = cols[1].text.strip() # ì²­ì•½ì¼
                price = cols[2].text.strip()    # ê³µëª¨ê°€
                host = cols[4].text.strip()     # ì£¼ê´€ì‚¬
                
                # ë‚ ì§œ ì •ë³´ê°€ ìˆëŠ” ìœ íš¨í•œ í–‰ë§Œ ì¶”ì¶œ
                if "~" in schedule:
                    ipo_list.append(f"- [KR] {name} | ì²­ì•½ì¼: {schedule} | ê³µëª¨ê°€: {price} | ì£¼ê´€ì‚¬: {host}")
            except:
                continue

        return ipo_list[:10] # ìµœì‹  10ê°œë§Œ
        
    except Exception as e:
        print(f"âš ï¸ í•œêµ­ IPO ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

def collect_us_ipo():
    """StockAnalysisì—ì„œ ë¯¸êµ­ IPO ìº˜ë¦°ë” ìˆ˜ì§‘"""
    print("ğŸ‡ºğŸ‡¸ ë¯¸êµ­ IPO ìº˜ë¦°ë”(StockAnalysis) ìˆ˜ì§‘ ì¤‘...")
    url = "https://stockanalysis.com/ipos/calendar/"
    
    try:
        res = requests.get(url, headers=HEADERS, timeout=10)
        soup = BeautifulSoup(res.text, "html.parser")
        
        # í…Œì´ë¸” ì°¾ê¸° (í´ë˜ìŠ¤ëª…ì€ ì‚¬ì´íŠ¸ ì—…ë°ì´íŠ¸ì— ë”°ë¼ ë³€í•  ìˆ˜ ìˆì–´ ë³´í¸ì ìœ¼ë¡œ íƒìƒ‰)
        tables = soup.find_all("table")
        if not tables:
            return []
            
        rows = tables[0].find_all("tr")[1:] # í—¤ë” ì œì™¸
        
        ipo_list = []
        for row in rows[:10]: # 10ê°œë§Œ
            cols = row.find_all("td")
            if len(cols) < 3: continue
            
            date = cols[0].text.strip()
            symbol = cols[1].text.strip()
            name = cols[2].text.strip()
            price = cols[3].text.strip() if len(cols) > 3 else "N/A"
            
            ipo_list.append(f"- [US] {name} ({symbol}) | ë‚ ì§œ: {date} | ì˜ˆìƒê°€: {price}")
            
        return ipo_list
        
    except Exception as e:
        print(f"âš ï¸ ë¯¸êµ­ IPO ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return []

def run_collector():
    korea_ipos = collect_korea_ipo()
    us_ipos = collect_us_ipo()
    
    all_ipos = korea_ipos + us_ipos
    
    if all_ipos:
        content = f"[Global IPO Calendar Update: {datetime.now().strftime('%Y-%m-%d')}]\n"
        content += "="*50 + "\n"
        content += "\n".join(all_ipos)
        
        # ì €ì¥
        save_path = os.path.join(SAVE_DIR, "ipo_calendar.txt")
        with open(save_path, "w", encoding="utf-8") as f:
            f.write(content)
            
        print(f"ğŸ‰ IPO ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(all_ipos)}ê±´ ì €ì¥ë¨.")
        print(f"ğŸ“‚ ì €ì¥ ê²½ë¡œ: {save_path}")
    else:
        print("â˜ï¸ ìˆ˜ì§‘ëœ ì˜ˆì • IPO ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    run_collector()